{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "from collections import Counter\n",
    "# from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, balanced_accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import mord\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.formula.api import ols\n",
    "import xgboost as xgb\n",
    "# from statsmodels.graphics.regressionplots import plot_regress_exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'review-Alaska_10.json'\n",
    "\n",
    "data_raw = pd.read_json(path, lines=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates the presence of NA values in the data. However, as this is one of the aspects we intend to investigate, we will selectively perform data cleaning at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "- index: The index of the data.\n",
    "- user_id: The ID of the reviewer.\n",
    "- name: The name of the reviewer.\n",
    "- time: The time of the review in Unix time format.\n",
    "- rating: The rating given by the reviewer for the business.\n",
    "- text: The text of the review.\n",
    "- pics: Pictures associated with the review.\n",
    "- resp: The business response to the review, including Unix time and the text of the response.\n",
    "- gmap_id: The ID of the business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nature of our research topic, which is to explore the influence of time of day on online ratings across different devices, we will be selecting specific data variables for further analysis. The data variables of interest include \"time,\" \"rating,\" and \"pics.\" The reason for selecting \"pics\" is due to the unfortunate inability to obtain data directly related to device types in the comments. Therefore, we need to make a crucial assumption: \n",
    "**we assume that comments with pictures are uploaded using mobile devices, while comments without pictures are uploaded using non-mobile devices.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "\n",
    "\n",
    "In this section, we will perform data preprocessing, which includes data cleaning and data transformation. Data cleaning involves handling missing values, outliers, and inconsistencies in the dataset. Data transformation may involve converting the \"pics\" data into device type data, etc. These steps allow us to make use of the available information and derive meaningful insights from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modified = (\n",
    "    data_raw\n",
    "    # Convert the timestamp to minutes since midnight\n",
    "    .assign(minutes_since_midnight=lambda x: pd.to_datetime(x['time'], unit='ms', utc=True)\n",
    "                                          .dt.tz_convert('America/Anchorage')\n",
    "                                          .dt.hour * 60 \n",
    "                                          + pd.to_datetime(x['time'], unit='ms', utc=True)\n",
    "                                          .dt.tz_convert('America/Anchorage')\n",
    "                                          .dt.minute)\n",
    "    # device[0,1] represents ['Non-mobile devices', 'Mobile devices']\n",
    "    .assign(device=lambda x: x['pics'].notnull().astype(int))\n",
    "    # rating_binary[0,1] represents ['Rating not equal to 5', 'Rating equal to 5']\n",
    "    .assign(rating_binary=lambda x: (x['rating'] == 5).astype(int))\n",
    "    # length of text (words)\n",
    "    .assign(text_length=lambda x: x['text'].apply(lambda t: len(t.split()) if t is not None else 0))\n",
    "    # number of pics\n",
    "    .assign(num_pics=lambda x: x['pics'].apply(lambda p: len(p) if isinstance(p, (list, pd.Series)) and p is not None else 0))\n",
    "    # has_resp[0,1] represents ['No response', 'Has response']\n",
    "    .assign(has_resp=lambda x: x['resp'].notnull().astype(int))\n",
    "    .filter(['minutes_since_midnight', 'rating','rating_binary', 'device', 'text_length', 'num_pics', 'has_resp'])\n",
    ")\n",
    "\n",
    "print(data_modified[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "(data_modified['minutes_since_midnight'] // 60).value_counts().sort_index().plot(kind='bar', alpha=0.7)\n",
    "plt.title('Distribution by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Minute-by-minute distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "data_modified['minutes_since_midnight'].value_counts().sort_index().plot(kind='bar', alpha=0.7)\n",
    "plt.title('Distribution by Minute of Day')\n",
    "plt.xlabel('Minute')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Adjust x-axis ticks for better visibility\n",
    "ticks = list(range(0, 1441, 120))  # Every 2 hours in minutes\n",
    "labels = [f\"{int(tick/60)}:{str(tick%60).zfill(2)}\" for tick in ticks]  # Convert minute ticks to HH:MM format\n",
    "plt.xticks(ticks, labels, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for data distribution by rating\n",
    "counts = data_modified['rating'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Rating')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 0.01 * counts.max(), f'{v / counts.sum() * 100:.1f}%', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for data distribution by rating\n",
    "counts = data_modified['rating_binary'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Rating (Binary)')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 0.01 * counts.max(), f'{v / counts.sum() * 100:.1f}%', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for data distribution by devices\n",
    "counts = data_modified['device'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Device')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Device')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 0.01 * counts.max(), f'{v / counts.sum() * 100:.1f}%', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin range for text_length\n",
    "bins_text = list(range(0, 101, 5)) + [data_modified['text_length'].max() + 1]\n",
    "\n",
    "# Labels for the bins\n",
    "labels = [f\"{bins_text[i]}-{bins_text[i+1]-1}\" for i in range(len(bins_text)-2)] + [\"100+\"]\n",
    "\n",
    "# Bin the text_length\n",
    "data_modified['text_length_binned'] = pd.cut(data_modified['text_length'], bins=bins_text, right=False, labels=labels)\n",
    "\n",
    "# Count the number of records in each bin\n",
    "counts_text = data_modified['text_length_binned'].value_counts().sort_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "counts_text.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Text Length')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for data distribution by num_pics\n",
    "max_pics = 5  # You can adjust this value based on your data\n",
    "data_modified['num_pics_binned'] = pd.cut(data_modified['num_pics'], bins=[-1, 1, 2, max_pics, data_modified['num_pics'].max()], labels=['0-1', '2', '3-'+str(max_pics), str(max_pics+1)+'+'])\n",
    "counts_pics = data_modified['num_pics_binned'].value_counts().sort_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "counts_pics.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Number of Pictures')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Number of Pictures')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for data distribution by has_resp\n",
    "counts = data_modified['has_resp'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Response')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Response')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 0.01 * counts.max(), f'{v / counts.sum() * 100:.1f}%', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modified = data_modified.filter(['minutes_since_midnight', 'rating', 'rating_binary' , 'device', 'text_length', 'num_pics', 'has_resp']).dropna()\n",
    "print(data_modified.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical variables\n",
    "categorical_vars = ['device', 'rating', 'rating_binary', 'has_resp']\n",
    "\n",
    "def detailed_chi2_pvalues(data, cat_vars):\n",
    "    results = []\n",
    "    for var1 in cat_vars:\n",
    "        for var2 in cat_vars:\n",
    "            if var1 != var2:\n",
    "                chi2_stat, p_val, dof, expected = stats.chi2_contingency(pd.crosstab(data[var1], data[var2]))\n",
    "                results.append({\n",
    "                    'Variable 1': var1,\n",
    "                    'Variable 2': var2,\n",
    "                    'Chi2 Statistic': chi2_stat,\n",
    "                    'P-value': p_val,\n",
    "                    'Degrees of Freedom': dof,\n",
    "                    'Minimum Expected Frequency': expected.min()\n",
    "                })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "detailed_chi2_df = detailed_chi2_pvalues(data_modified, categorical_vars)\n",
    "detailed_chi2_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify continuous variables\n",
    "continuous_vars = ['minutes_since_midnight', 'text_length', 'num_pics']\n",
    "\n",
    "def detailed_spearman_correlation(data, cont_vars):\n",
    "    results = []\n",
    "    for var1 in cont_vars:\n",
    "        for var2 in cont_vars:\n",
    "            if var1 != var2:\n",
    "                corr, p_val = stats.spearmanr(data[var1], data[var2])\n",
    "                results.append({\n",
    "                    'Variable 1': var1,\n",
    "                    'Variable 2': var2,\n",
    "                    'Spearman Correlation': corr,\n",
    "                    'P-value': p_val\n",
    "                })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "detailed_spearman_df = detailed_spearman_correlation(data_modified, continuous_vars)\n",
    "detailed_spearman_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_modified[['minutes_since_midnight', 'rating', 'rating_binary', 'device', 'text_length', 'num_pics', 'has_resp']]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Features'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(\"\\nMulticollinearity test:\")\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y) in the balanced dataset\n",
    "X = data_modified.drop(['rating', 'rating_binary'], axis=1)\n",
    "y = data_modified['rating']\n",
    "\n",
    "X_bi = data_modified.drop(['rating', 'rating_binary'], axis=1)\n",
    "y_bi = data_modified['rating_binary']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2221877)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices\n",
    "train_idx, test_idx = train_test_split(data_modified.index, test_size=0.2, random_state=2221877)\n",
    "\n",
    "# Use indices to extract training and testing data\n",
    "X_train = data_modified.loc[train_idx].drop(['rating', 'rating_binary'], axis=1)\n",
    "y_train = data_modified.loc[train_idx]['rating']\n",
    "\n",
    "X_train_bi = data_modified.loc[train_idx].drop('rating', axis=1)\n",
    "y_train_bi = data_modified.loc[train_idx]['rating_binary']\n",
    "\n",
    "X_test = data_modified.loc[test_idx].drop(['rating', 'rating_binary'], axis=1)\n",
    "y_test = data_modified.loc[test_idx]['rating']\n",
    "\n",
    "X_test_bi = data_modified.loc[test_idx].drop('rating', axis=1)\n",
    "y_test_bi = data_modified.loc[test_idx]['rating_binary']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Balancing\n",
    "\n",
    "It can be seen that there is a significant data imbalance between the different ratings. Having tried sampling and undersampling, I ended up combining them to try and get the most optimal sampling results to support my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of samples in each class\n",
    "counter = Counter(y)\n",
    "counter_bi = Counter(y_bi)\n",
    "\n",
    "# Define the target sample numbers for over-sampling and under-sampling\n",
    "max_samples = max(counter.values())\n",
    "min_samples = min(counter.values())\n",
    "max_samples_bi = max(counter_bi.values())\n",
    "min_samples_bi = min(counter_bi.values())\n",
    "\n",
    "# Define pipeline\n",
    "over = SMOTE(sampling_strategy={class_label: max_samples for class_label in counter})\n",
    "under = RandomUnderSampler(sampling_strategy={class_label: min_samples for class_label in counter})\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "over_bi = SMOTE(sampling_strategy={class_label: max_samples_bi for class_label in counter_bi})\n",
    "under_bi = RandomUnderSampler(sampling_strategy={class_label: min_samples_bi for class_label in counter_bi})\n",
    "steps_bi = [('o', over_bi), ('u', under_bi)]\n",
    "pipeline_bi = Pipeline(steps=steps_bi)\n",
    "\n",
    "# Apply the pipeline\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "X_resampled_bi, y_resampled_bi = pipeline_bi.fit_resample(X_bi, y_bi)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=2221877)\n",
    "X_train_bi, X_test_bi, y_train_bi, y_test_bi = train_test_split(X_resampled_bi, y_resampled_bi, test_size=0.2, random_state=2221877)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of samples in each class before resampling\n",
    "print(\"Sample numbers before resampling:\", counter)\n",
    "\n",
    "# Calculate the number of samples in each class after resampling\n",
    "counter_after = Counter(y_resampled)\n",
    "print(\"Sample numbers after resampling:\", counter_after)\n",
    "\n",
    "# Visualize class distribution before resampling\n",
    "plt.bar(counter.keys(), counter.values())\n",
    "plt.xlabel('Rating Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution Before Resampling')\n",
    "plt.show()\n",
    "\n",
    "# Visualize class distribution after resampling\n",
    "plt.bar(counter_after.keys(), counter_after.values())\n",
    "plt.xlabel('Rating Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution After Resampling')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of samples in each class before resampling\n",
    "print(\"Sample numbers before resampling:\", counter_bi)\n",
    "\n",
    "# Calculate the number of samples in each class after resampling\n",
    "counter_after_bi = Counter(y_resampled_bi)\n",
    "print(\"Sample numbers after resampling:\", counter_after_bi)\n",
    "\n",
    "# Visualize class distribution before resampling\n",
    "plt.bar(counter_bi.keys(), counter_bi.values())\n",
    "plt.xlabel('Rating Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution Before Resampling')\n",
    "plt.show()\n",
    "\n",
    "# Visualize class distribution after resampling\n",
    "plt.bar(counter_after_bi.keys(), counter_after_bi.values())\n",
    "plt.xlabel('Rating Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution After Resampling')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT = DecisionTreeClassifier()\n",
    "\n",
    "model_DT.fit(X_resampled, y_resampled)\n",
    "\n",
    "predictions_DT = model_DT.predict(X_test)\n",
    "\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test, predictions_DT))\n",
    "print(classification_report(y_test, predictions_DT))\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_DT)\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Explainer object\n",
    "explainer_DT = shap.Explainer(model_DT)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values_DT = explainer_DT.shap_values(X_test)\n",
    "\n",
    "# Visualise the first prediction's explanation\n",
    "shap.summary_plot(shap_values_DT, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiclass_summary(shap_values_list, X_test, class_names=None, max_display=None):\n",
    "    num_classes = len(shap_values_list)\n",
    "    \n",
    "    if max_display is None:\n",
    "        max_display = num_classes\n",
    "    \n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class {i}\" for i in range(num_classes)]\n",
    "    \n",
    "    for i in range(max_display):\n",
    "        shap_values_class = shap_values_list[i]\n",
    "        class_name = class_names[i]\n",
    "        \n",
    "        shap.summary_plot(shap_values_class, X_test, show=False, plot_type='dot', title=class_name)\n",
    "        plt.title(class_name)\n",
    "        plt.show()\n",
    "\n",
    "# Call the function with your shap_values_DT and X_test\n",
    "plot_multiclass_summary(shap_values_DT, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT_bi = DecisionTreeClassifier()\n",
    "\n",
    "model_DT_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "predictions_DT_bi = model_DT_bi.predict(X_test_bi)\n",
    "\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test_bi, predictions_DT_bi))\n",
    "print(classification_report(y_test_bi, predictions_DT_bi))\n",
    "\n",
    "cm_bi = confusion_matrix(y_test_bi, predictions_DT_bi)\n",
    "sns.heatmap(cm_bi, annot=True, fmt=\".0f\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Explainer object\n",
    "explainer_DT_bi = shap.Explainer(model_DT_bi)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values_DT_bi = explainer_DT_bi.shap_values(X_test_bi)\n",
    "\n",
    "# Visualise the first prediction's explanation\n",
    "shap.summary_plot(shap_values_DT_bi, X_test_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = RandomForestClassifier()\n",
    "\n",
    "model_RF.fit(X_resampled, y_resampled)\n",
    "\n",
    "predictions_RF = model_RF.predict(X_test)\n",
    "\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test, predictions_RF))\n",
    "print(classification_report(y_test, predictions_RF))\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_RF)\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF_bi = RandomForestClassifier()\n",
    "\n",
    "model_RF_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "predictions_RF_bi = model_RF_bi.predict(X_test_bi)\n",
    "\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test_bi, predictions_RF_bi))\n",
    "print(classification_report(y_test_bi, predictions_RF_bi))\n",
    "\n",
    "cm_bi = confusion_matrix(y_test_bi, predictions_RF_bi)\n",
    "sns.heatmap(cm_bi, annot=True, fmt=\".0f\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LogisticAT model\n",
    "model_OL = mord.LogisticAT()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_OL.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Use the model to make predictions\n",
    "predictions_OL = model_OL.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test, predictions_OL))\n",
    "print(classification_report(y_test, predictions_OL))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions_OL)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LogisticAT model\n",
    "model_OL_bi = mord.LogisticAT()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_OL_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "# Use the model to make predictions\n",
    "predictions_OL_bi = model_OL_bi.predict(X_test_bi)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test_bi, predictions_OL_bi))\n",
    "print(classification_report(y_test_bi, predictions_OL_bi))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_bi = confusion_matrix(y_test_bi, predictions_OL_bi)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_bi, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GradientBoostingClassifier\n",
    "model_GB = GradientBoostingClassifier()\n",
    "\n",
    "model_GB.fit(X_resampled, y_resampled)\n",
    "\n",
    "predictions_GB = model_GB.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test, predictions_GB))\n",
    "print(classification_report(y_test, predictions_GB))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions_GB)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GradientBoostingClassifier\n",
    "model_GB_bi = GradientBoostingClassifier()\n",
    "\n",
    "model_GB_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "predictions_GB_bi = model_GB_bi.predict(X_test_bi)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test_bi, predictions_GB_bi))\n",
    "print(classification_report(y_test_bi, predictions_GB_bi))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_bi = confusion_matrix(y_test_bi, predictions_GB_bi)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_bi, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier\n",
    "model_XGB = xgb.XGBClassifier()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_XGB.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions\n",
    "predictions_XGB = model_XGB.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score (XGBoost): ', balanced_accuracy_score(y_test, predictions_XGB))\n",
    "print(classification_report(y_test, predictions_XGB))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_XGB = confusion_matrix(y_test, predictions_XGB)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_XGB, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (XGBoost)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier\n",
    "model_XGB_bi = xgb.XGBClassifier()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_XGB_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "# Make predictions\n",
    "predictions_XGB_bi = model_XGB_bi.predict(X_test_bi)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score (XGBoost): ', balanced_accuracy_score(y_test_bi, predictions_XGB_bi))\n",
    "print(classification_report(y_test_bi, predictions_XGB_bi))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_XGB_bi = confusion_matrix(y_test_bi, predictions_XGB_bi)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_XGB_bi, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (XGBoost)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical features\n",
    "cat_features = ['device', 'has_resp']\n",
    "\n",
    "# Create a CatBoost classifier\n",
    "model_CB = cb.CatBoostClassifier(cat_features=cat_features, verbose=0)\n",
    "\n",
    "# Fit the model to the data\n",
    "model_CB.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions\n",
    "predictions_CB = model_CB.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test, predictions_CB))\n",
    "print(classification_report(y_test, predictions_CB))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_CB = confusion_matrix(y_test, predictions_CB)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_CB, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (CatBoost)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CatBoost classifier\n",
    "model_CB_bi = cb.CatBoostClassifier(cat_features=cat_features, verbose=0)\n",
    "\n",
    "# Fit the model to the data\n",
    "model_CB_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "# Make predictions\n",
    "predictions_CB_bi = model_CB_bi.predict(X_test_bi)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test_bi, predictions_CB_bi))\n",
    "print(classification_report(y_test_bi, predictions_CB_bi))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_CB_bi = confusion_matrix(y_test_bi, predictions_CB_bi)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_CB_bi, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (CatBoost)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for all models\n",
    "probs_DT = model_DT.predict_proba(X_test)[:, 1]\n",
    "probs_RF = model_RF.predict_proba(X_test)[:, 1]\n",
    "probs_GB = model_GB.predict_proba(X_test)[:, 1]\n",
    "probs_OL = model_OL.predict_proba(X_test)[:, 1]\n",
    "probs_XGB = model_XGB.predict_proba(X_test)[:, 1]\n",
    "probs_CB = model_CB.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each model\n",
    "fpr_DT, tpr_DT, _ = roc_curve(y_test, probs_DT)\n",
    "roc_auc_DT = auc(fpr_DT, tpr_DT)\n",
    "\n",
    "fpr_RF, tpr_RF, _ = roc_curve(y_test, probs_RF)\n",
    "roc_auc_RF = auc(fpr_RF, tpr_RF)\n",
    "\n",
    "fpr_GB, tpr_GB, _ = roc_curve(y_test, probs_GB)\n",
    "roc_auc_GB = auc(fpr_GB, tpr_GB)\n",
    "\n",
    "fpr_OL, tpr_OL, _ = roc_curve(y_test, probs_OL)\n",
    "roc_auc_OL = auc(fpr_OL, tpr_OL)\n",
    "\n",
    "fpr_XGB, tpr_XGB, _ = roc_curve(y_test, probs_XGB)\n",
    "roc_auc_XGB = auc(fpr_XGB, tpr_XGB)\n",
    "\n",
    "fpr_CB, tpr_CB, _ = roc_curve(y_test, probs_CB)\n",
    "roc_auc_CB = auc(fpr_CB, tpr_CB)\n",
    "\n",
    "# Plot ROC curves in the same plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(fpr_DT, tpr_DT, color='darkorange', lw=lw, label=f'Decision Tree (AUC = {roc_auc_DT:0.2f})')\n",
    "plt.plot(fpr_RF, tpr_RF, color='blue', lw=lw, label=f'Random Forest (AUC = {roc_auc_RF:0.2f})')\n",
    "plt.plot(fpr_GB, tpr_GB, color='green', lw=lw, label=f'Gradient Boosting (AUC = {roc_auc_GB:0.2f})')\n",
    "plt.plot(fpr_OL, tpr_OL, color='red', lw=lw, label=f'Ordered Logit (AUC = {roc_auc_OL:0.2f})')\n",
    "plt.plot(fpr_XGB, tpr_XGB, color='purple', lw=lw, label=f'XGBoost (AUC = {roc_auc_XGB:0.2f})')\n",
    "plt.plot(fpr_CB, tpr_CB, color='cyan', lw=lw, label=f'CatBoost (AUC = {roc_auc_CB:0.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print out AUC scores\n",
    "print(\"Decision Tree AUC:\", roc_auc_score(y_test, probs_DT))\n",
    "print(\"Random Forest AUC:\", roc_auc_score(y_test, probs_RF))\n",
    "print(\"Gradient Boosting AUC:\", roc_auc_score(y_test, probs_GB))\n",
    "print(\"Ordered Logit AUC:\", roc_auc_score(y_test, probs_OL))\n",
    "print(\"XGBoost AUC:\", roc_auc_score(y_test, probs_XGB))\n",
    "print(\"CatBoost AUC:\", roc_auc_score(y_test, probs_CB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc(y, probs, n_iterations=1000):\n",
    "    \"\"\" Calculate bootstrap AUC scores. \"\"\"\n",
    "    auc_scores = []\n",
    "    y = y.to_numpy()  # Convert to numpy array\n",
    "    for _ in range(n_iterations):\n",
    "        # Bootstrapped indices\n",
    "        indices = resample(np.arange(len(y)))\n",
    "        if len(np.unique(y[indices])) < 2:\n",
    "            # We need at least one positive and one negative sample for ROC AUC\n",
    "            continue\n",
    "        auc_score = roc_auc_score(y[indices], probs[indices])\n",
    "        auc_scores.append(auc_score)\n",
    "    return auc_scores\n",
    "\n",
    "# Get bootstrapped AUC scores\n",
    "auc_scores_DT = bootstrap_auc(y_test, probs_DT)\n",
    "auc_scores_RF = bootstrap_auc(y_test, probs_RF)\n",
    "auc_scores_GB = bootstrap_auc(y_test, probs_GB)\n",
    "auc_scores_OL = bootstrap_auc(y_test, probs_OL)\n",
    "auc_scores_XGB = bootstrap_auc(y_test, probs_XGB)\n",
    "auc_scores_CB = bootstrap_auc(y_test, probs_CB)\n",
    "\n",
    "# Calculate 95% CI for AUC\n",
    "conf_int_DT = np.percentile(auc_scores_DT, [2.5, 97.5])\n",
    "conf_int_RF = np.percentile(auc_scores_RF, [2.5, 97.5])\n",
    "conf_int_GB = np.percentile(auc_scores_GB, [2.5, 97.5])\n",
    "conf_int_OL = np.percentile(auc_scores_OL, [2.5, 97.5])\n",
    "conf_int_XGB = np.percentile(auc_scores_XGB, [2.5, 97.5])\n",
    "conf_int_CB = np.percentile(auc_scores_CB, [2.5, 97.5])\n",
    "\n",
    "print(f\"Decision Tree AUC: {np.mean(auc_scores_DT):.4f} (95% CI: {conf_int_DT[0]:.4f}, {conf_int_DT[1]:.4f})\")\n",
    "print(f\"Random Forest AUC: {np.mean(auc_scores_RF):.4f} (95% CI: {conf_int_RF[0]:.4f}, {conf_int_RF[1]:.4f})\")\n",
    "print(f\"Gradient Boosting AUC: {np.mean(auc_scores_GB):.4f} (95% CI: {conf_int_GB[0]:.4f}, {conf_int_GB[1]:.4f})\")\n",
    "print(f\"Ordered Logit AUC: {np.mean(auc_scores_OL):.4f} (95% CI: {conf_int_OL[0]:.4f}, {conf_int_OL[1]:.4f})\")\n",
    "print(f\"XGBoost AUC: {np.mean(auc_scores_XGB):.4f} (95% CI: {conf_int_XGB[0]:.4f}, {conf_int_XGB[1]:.4f})\")\n",
    "print(f\"CatBoost AUC: {np.mean(auc_scores_CB):.4f} (95% CI: {conf_int_CB[0]:.4f}, {conf_int_CB[1]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for all models\n",
    "probs_DT_bi = model_DT_bi.predict_proba(X_test_bi)[:, 1]\n",
    "probs_RF_bi = model_RF_bi.predict_proba(X_test_bi)[:, 1]\n",
    "probs_GB_bi = model_GB_bi.predict_proba(X_test_bi)[:, 1]\n",
    "probs_OL_bi = model_OL_bi.predict_proba(X_test_bi)[:, 1]\n",
    "probs_XGB_bi = model_XGB_bi.predict_proba(X_test_bi)[:, 1]\n",
    "probs_CB_bi = model_CB_bi.predict_proba(X_test_bi)[:, 1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each model\n",
    "fpr_DT, tpr_DT, _ = roc_curve(y_test_bi, probs_DT_bi)\n",
    "roc_auc_DT = auc(fpr_DT, tpr_DT)\n",
    "\n",
    "fpr_RF, tpr_RF, _ = roc_curve(y_test_bi, probs_RF_bi)\n",
    "roc_auc_RF = auc(fpr_RF, tpr_RF)\n",
    "\n",
    "fpr_GB, tpr_GB, _ = roc_curve(y_test_bi, probs_GB_bi)\n",
    "roc_auc_GB = auc(fpr_GB, tpr_GB)\n",
    "\n",
    "fpr_OL, tpr_OL, _ = roc_curve(y_test_bi, probs_OL_bi)\n",
    "roc_auc_OL = auc(fpr_OL, tpr_OL)\n",
    "\n",
    "fpr_XGB, tpr_XGB, _ = roc_curve(y_test_bi, probs_XGB_bi)\n",
    "roc_auc_XGB = auc(fpr_XGB, tpr_XGB)\n",
    "\n",
    "fpr_CB, tpr_CB, _ = roc_curve(y_test_bi, probs_CB_bi)\n",
    "roc_auc_CB = auc(fpr_CB, tpr_CB)\n",
    "\n",
    "# Plot ROC curves in the same plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(fpr_DT, tpr_DT, color='darkorange', lw=lw, label=f'Decision Tree (AUC = {roc_auc_DT:0.2f})')\n",
    "plt.plot(fpr_RF, tpr_RF, color='blue', lw=lw, label=f'Random Forest (AUC = {roc_auc_RF:0.2f})')\n",
    "plt.plot(fpr_GB, tpr_GB, color='green', lw=lw, label=f'Gradient Boosting (AUC = {roc_auc_GB:0.2f})')\n",
    "plt.plot(fpr_OL, tpr_OL, color='red', lw=lw, label=f'Ordered Logit (AUC = {roc_auc_OL:0.2f})')\n",
    "plt.plot(fpr_XGB, tpr_XGB, color='purple', lw=lw, label=f'XGBoost (AUC = {roc_auc_XGB:0.2f})')\n",
    "plt.plot(fpr_CB, tpr_CB, color='cyan', lw=lw, label=f'CatBoost (AUC = {roc_auc_CB:0.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC Binary)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print out AUC scores\n",
    "print(\"Decision Tree AUC:\", roc_auc_score(y_test, probs_DT))\n",
    "print(\"Random Forest AUC:\", roc_auc_score(y_test, probs_RF))\n",
    "print(\"Gradient Boosting AUC:\", roc_auc_score(y_test, probs_GB))\n",
    "print(\"Ordered Logit AUC:\", roc_auc_score(y_test, probs_OL))\n",
    "print(\"XGBoost AUC:\", roc_auc_score(y_test, probs_XGB))\n",
    "print(\"CatBoost AUC:\", roc_auc_score(y_test, probs_CB))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
