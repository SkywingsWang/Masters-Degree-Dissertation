{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import catboost as cb\n",
    "from collections import Counter\n",
    "# from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, balanced_accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import mord\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.formula.api import ols\n",
    "# import xgboost as xgb\n",
    "# from statsmodels.graphics.regressionplots import plot_regress_exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'review-Alaska_10.json'\n",
    "\n",
    "data_raw = pd.read_json(path, lines=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates the presence of NA values in the data. However, as this is one of the aspects we intend to investigate, we will selectively perform data cleaning at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "- index: The index of the data.\n",
    "- user_id: The ID of the reviewer.\n",
    "- name: The name of the reviewer.\n",
    "- time: The time of the review in Unix time format.\n",
    "- rating: The rating given by the reviewer for the business.\n",
    "- text: The text of the review.\n",
    "- pics: Pictures associated with the review.\n",
    "- resp: The business response to the review, including Unix time and the text of the response.\n",
    "- gmap_id: The ID of the business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nature of our research topic, which is to explore the influence of time of day on online ratings across different devices, we will be selecting specific data variables for further analysis. The data variables of interest include \"time,\" \"rating,\" and \"pics.\" The reason for selecting \"pics\" is due to the unfortunate inability to obtain data directly related to device types in the comments. Therefore, we need to make a crucial assumption: \n",
    "**we assume that comments with pictures are uploaded using mobile devices, while comments without pictures are uploaded using non-mobile devices.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "\n",
    "\n",
    "In this section, we will perform data preprocessing, which includes data cleaning and data transformation. Data cleaning involves handling missing values, outliers, and inconsistencies in the dataset. Data transformation may involve converting the \"pics\" data into device type data, etc. These steps allow us to make use of the available information and derive meaningful insights from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modified = (\n",
    "    data_raw\n",
    "    # Convert the timestamp to minutes since midnight\n",
    "    .assign(minutes_since_midnight=lambda x: pd.to_datetime(x['time'], unit='ms', utc=True)\n",
    "                                          .dt.tz_convert('America/Anchorage')\n",
    "                                          .dt.hour * 60 \n",
    "                                          + pd.to_datetime(x['time'], unit='ms', utc=True)\n",
    "                                          .dt.tz_convert('America/Anchorage')\n",
    "                                          .dt.minute)\n",
    "    # device[0,1] represents ['Non-mobile devices', 'Mobile devices']\n",
    "    .assign(device=lambda x: x['pics'].notnull().astype(int))\n",
    "    # rating_binary[0,1] represents ['Rating not equal to 5', 'Rating equal to 5']\n",
    "    .assign(rating_binary=lambda x: (x['rating'] == 5).astype(int))\n",
    "    # length of text (words)\n",
    "    .assign(text_length=lambda x: x['text'].apply(lambda t: len(t.split()) if t is not None else 0))\n",
    "    # number of pics\n",
    "    .assign(num_pics=lambda x: x['pics'].apply(lambda p: len(p) if isinstance(p, (list, pd.Series)) and p is not None else 0))\n",
    "    # has_resp[0,1] represents ['No response', 'Has response']\n",
    "    .assign(has_resp=lambda x: x['resp'].notnull().astype(int))\n",
    "    .filter(['minutes_since_midnight', 'rating','rating_binary', 'device', 'text_length', 'num_pics', 'has_resp'])\n",
    ").dropna()\n",
    "\n",
    "print(data_modified[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modified.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "(data_modified['minutes_since_midnight'] // 60).value_counts().sort_index().plot(kind='bar', alpha=0.7)\n",
    "plt.title('Distribution by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Minute-by-minute distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "data_modified['minutes_since_midnight'].value_counts().sort_index().plot(kind='bar', alpha=0.7)\n",
    "plt.title('Distribution by Minute of Day')\n",
    "plt.xlabel('Minute')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Adjust x-axis ticks for better visibility\n",
    "ticks = list(range(0, 1441, 120))  # Every 2 hours in minutes\n",
    "labels = [f\"{int(tick/60)}:{str(tick%60).zfill(2)}\" for tick in ticks]  # Convert minute ticks to HH:MM format\n",
    "plt.xticks(ticks, labels, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for data distribution by rating\n",
    "counts = data_modified['rating'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Rating')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 0.01 * counts.max(), f'{v / counts.sum() * 100:.1f}%', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for data distribution by rating\n",
    "counts = data_modified['rating_binary'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Rating (Binary)')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 0.01 * counts.max(), f'{v / counts.sum() * 100:.1f}%', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for data distribution by devices\n",
    "counts = data_modified['device'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Device')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Device')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 0.01 * counts.max(), f'{v / counts.sum() * 100:.1f}%', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin range for text_length\n",
    "bins_text = list(range(0, 101, 5)) + [data_modified['text_length'].max() + 1]\n",
    "\n",
    "# Labels for the bins\n",
    "labels = [f\"{bins_text[i]}-{bins_text[i+1]-1}\" for i in range(len(bins_text)-2)] + [\"100+\"]\n",
    "\n",
    "# Bin the text_length\n",
    "data_modified['text_length_binned'] = pd.cut(data_modified['text_length'], bins=bins_text, right=False, labels=labels)\n",
    "\n",
    "# Count the number of records in each bin\n",
    "counts_text = data_modified['text_length_binned'].value_counts().sort_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "counts_text.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Text Length')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for data distribution by num_pics\n",
    "max_pics = 5  \n",
    "data_modified['num_pics_binned'] = pd.cut(data_modified['num_pics'], bins=[-1, 1, 2, max_pics, data_modified['num_pics'].max()], labels=['0-1', '2', '3-'+str(max_pics), str(max_pics+1)+'+'])\n",
    "counts_pics = data_modified['num_pics_binned'].value_counts().sort_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "counts_pics.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Number of Pictures')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Number of Pictures')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for data distribution by has_resp\n",
    "counts = data_modified['has_resp'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Response')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Response')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 0.01 * counts.max(), f'{v / counts.sum() * 100:.1f}%', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modified = data_modified.filter(['minutes_since_midnight', 'rating', 'rating_binary' , 'device', 'text_length', 'num_pics', 'has_resp'])\n",
    "print(data_modified.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_plot = ['minutes_since_midnight', 'device', 'text_length', 'num_pics', 'has_resp']\n",
    "\n",
    "# Create subplots for each column and for both 'rating' and 'rating_binary'\n",
    "fig, axs = plt.subplots(len(columns_to_plot), 2, figsize=(15, 5*len(columns_to_plot)))\n",
    "\n",
    "# For each column in the list\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    \n",
    "    # Bar plot for 'rating'\n",
    "    sns.barplot(x=col, y='rating', data=data_modified, ax=axs[i, 0], errorbar=None)\n",
    "    axs[i, 0].set_title(f'Mean Rating by {col}')\n",
    "    \n",
    "    # Simplify x-axis labels if too many unique values\n",
    "    if len(data_modified[col].unique()) > 20:\n",
    "        ticks = axs[i, 0].get_xticks()\n",
    "        visible_ticks = ticks[::5]  # Show every 5th tick\n",
    "        axs[i, 0].set_xticks(visible_ticks)\n",
    "        axs[i, 0].set_xticklabels(data_modified[col].unique()[::5])\n",
    "    \n",
    "    # Bar plot for 'rating_binary'\n",
    "    sns.barplot(x=col, y='rating_binary', data=data_modified, ax=axs[i, 1], errorbar=None)\n",
    "    axs[i, 1].set_title(f'Mean Rating Binary by {col}')\n",
    "    \n",
    "    # Simplify x-axis labels if too many unique values\n",
    "    if len(data_modified[col].unique()) > 20:\n",
    "        ticks = axs[i, 1].get_xticks()\n",
    "        visible_ticks = ticks[::5]  # Show every 5th tick\n",
    "        axs[i, 1].set_xticks(visible_ticks)\n",
    "        axs[i, 1].set_xticklabels(data_modified[col].unique()[::5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minutes_since_midnight and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the number of reviews and average rating per year\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Twin the axes for two different y-axes\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Convert the timestamp to a readable date format with correct timezone\n",
    "date = pd.to_datetime(data_raw['time'], unit='ms', utc=True).dt.tz_convert('America/Anchorage')\n",
    "year = date.dt.year\n",
    "\n",
    "# Plotting the number of reviews per year\n",
    "sns.countplot(x=year, ax=ax1, order=sorted(year.unique()), color='skyblue')\n",
    "\n",
    "# Plotting the average rating per year\n",
    "sns.pointplot(x=year, y=data_modified['rating'], ax=ax2, order=sorted(year.unique()), color='salmon', errorbar=None)\n",
    "\n",
    "# Setting the y axis labels\n",
    "ax1.set_ylabel('Number of Reviews', color='blue')\n",
    "ax2.set_ylabel('Average Rating', color='red')\n",
    "\n",
    "plt.title('Number of Reviews and Average Rating per Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by minutes_since_midnight and calculating the average rating\n",
    "average_ratings = data_modified.groupby('minutes_since_midnight')['rating'].mean().reset_index()\n",
    "\n",
    "# Plotting the average rating throughout the day\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=average_ratings, x='minutes_since_midnight', y='rating', color=\"salmon\")\n",
    "plt.title('Average Rating Throughout the Day')\n",
    "plt.xlabel('Minutes Since Midnight')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### device and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plot showing distribution of ratings based on device type\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(data=data_modified[data_modified['device'] == 0]['rating'], label=\"Non-mobile devices\", fill=True)\n",
    "sns.kdeplot(data=data_modified[data_modified['device'] == 1]['rating'], label=\"Mobile devices\", fill=True)\n",
    "plt.title('Distribution of Ratings Based on Device Type')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text length and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the boxplot for text_length vs rating\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data_modified, x='rating', y='text_length', palette=\"pastel\")\n",
    "plt.title('Boxplot of Text Length vs Rating')\n",
    "plt.ylim(0, 1000)  # Limiting y-axis for a clearer view\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of pics and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a boxplot to show the distribution of pic count for each rating\n",
    "plt.figure(figsize=(12, 6))\n",
    "data_modified.boxplot(column='num_pics', by='rating', grid=False, vert=False, patch_artist=True)\n",
    "plt.title('Distribution of Picture Count by Rating')\n",
    "plt.xlabel('Picture Count')\n",
    "plt.ylabel('Rating')\n",
    "plt.suptitle('')  # Suppress the default suptitle\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### has_resp and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Count of reviews by rating and response status with rating on y-axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=data_modified, y='rating', hue='has_resp', palette=\"pastel\")\n",
    "plt.title('Count of Reviews by Rating and Response Status')\n",
    "plt.ylabel('Rating')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.legend(title='Has Response', labels=['No Response', 'Has Response'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting the boxplot for has_resp vs rating\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.boxplot(data=data_modified, x='rating', y='has_resp', palette=\"pastel\", orient=\"h\")\n",
    "# plt.title('Boxplot of Whether Merchant Responded vs Rating')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "correlation_matrix = data_modified.corr()\n",
    "\n",
    "# Extract correlation with 'rating' and 'rating_binary'\n",
    "correlation_with_rating = correlation_matrix[['rating', 'rating_binary']].drop(['rating', 'rating_binary'])\n",
    "\n",
    "correlation_with_rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical variables\n",
    "categorical_vars = ['device', 'rating', 'rating_binary', 'has_resp']\n",
    "\n",
    "def detailed_chi2_pvalues(data, cat_vars):\n",
    "    results = []\n",
    "    for var1 in cat_vars:\n",
    "        for var2 in cat_vars:\n",
    "            if var1 != var2:\n",
    "                chi2_stat, p_val, dof, expected = stats.chi2_contingency(pd.crosstab(data[var1], data[var2]))\n",
    "                results.append({\n",
    "                    'Variable 1': var1,\n",
    "                    'Variable 2': var2,\n",
    "                    'Chi2 Statistic': chi2_stat,\n",
    "                    'P-value': p_val,\n",
    "                    'Degrees of Freedom': dof,\n",
    "                    'Minimum Expected Frequency': expected.min()\n",
    "                })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "detailed_chi2_df = detailed_chi2_pvalues(data_modified, categorical_vars)\n",
    "detailed_chi2_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify continuous variables\n",
    "continuous_vars = ['minutes_since_midnight', 'text_length', 'num_pics']\n",
    "\n",
    "def detailed_spearman_correlation(data, cont_vars):\n",
    "    results = []\n",
    "    for var1 in cont_vars:\n",
    "        for var2 in cont_vars:\n",
    "            if var1 != var2:\n",
    "                corr, p_val = stats.spearmanr(data[var1], data[var2])\n",
    "                results.append({\n",
    "                    'Variable 1': var1,\n",
    "                    'Variable 2': var2,\n",
    "                    'Spearman Correlation': corr,\n",
    "                    'P-value': p_val\n",
    "                })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "detailed_spearman_df = detailed_spearman_correlation(data_modified, continuous_vars)\n",
    "detailed_spearman_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_modified[['minutes_since_midnight', 'rating', 'rating_binary', 'device', 'text_length', 'num_pics', 'has_resp']]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Features'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(\"\\nMulticollinearity test:\")\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y) in the balanced dataset\n",
    "X = data_modified.drop(['rating', 'rating_binary'], axis=1)\n",
    "y = data_modified['rating']\n",
    "\n",
    "X_bi = data_modified.drop(['rating', 'rating_binary'], axis=1)\n",
    "y_bi = data_modified['rating_binary']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2221877)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices\n",
    "train_idx, test_idx = train_test_split(data_modified.index, test_size=0.2, random_state=2221877)\n",
    "\n",
    "# Use indices to extract training and testing data\n",
    "X_train = data_modified.loc[train_idx].drop(['rating', 'rating_binary'], axis=1)\n",
    "y_train = data_modified.loc[train_idx]['rating']\n",
    "\n",
    "X_train_bi = data_modified.loc[train_idx].drop('rating', axis=1)\n",
    "y_train_bi = data_modified.loc[train_idx]['rating_binary']\n",
    "\n",
    "X_test = data_modified.loc[test_idx].drop(['rating', 'rating_binary'], axis=1)\n",
    "y_test = data_modified.loc[test_idx]['rating']\n",
    "\n",
    "X_test_bi = data_modified.loc[test_idx].drop('rating', axis=1)\n",
    "y_test_bi = data_modified.loc[test_idx]['rating_binary']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Balancing\n",
    "\n",
    "It can be seen that there is a significant data imbalance between the different ratings. Having tried sampling and undersampling, I ended up combining them to try and get the most optimal sampling results to support my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of samples in each class\n",
    "counter = Counter(y)\n",
    "counter_bi = Counter(y_bi)\n",
    "\n",
    "# Define the target sample numbers for over-sampling and under-sampling\n",
    "max_samples = max(counter.values())\n",
    "min_samples = min(counter.values())\n",
    "max_samples_bi = max(counter_bi.values())\n",
    "min_samples_bi = min(counter_bi.values())\n",
    "\n",
    "# Define pipeline\n",
    "over = SMOTE(sampling_strategy={class_label: max_samples for class_label in counter})\n",
    "under = RandomUnderSampler(sampling_strategy={class_label: min_samples for class_label in counter})\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "over_bi = SMOTE(sampling_strategy={class_label: max_samples_bi for class_label in counter_bi})\n",
    "under_bi = RandomUnderSampler(sampling_strategy={class_label: min_samples_bi for class_label in counter_bi})\n",
    "steps_bi = [('o', over_bi), ('u', under_bi)]\n",
    "pipeline_bi = Pipeline(steps=steps_bi)\n",
    "\n",
    "# Apply the pipeline\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "X_resampled_bi, y_resampled_bi = pipeline_bi.fit_resample(X_bi, y_bi)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=2221877)\n",
    "X_train_bi, X_test_bi, y_train_bi, y_test_bi = train_test_split(X_resampled_bi, y_resampled_bi, test_size=0.2, random_state=2221877)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count samples for each class in 'rating' and 'rating_binary'\n",
    "count_rating = Counter(y_resampled)\n",
    "count_rating_bi = Counter(y_resampled_bi)\n",
    "\n",
    "count_rating, count_rating_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT = DecisionTreeClassifier()\n",
    "\n",
    "model_DT.fit(X_resampled, y_resampled)\n",
    "\n",
    "predictions_DT = model_DT.predict(X_test)\n",
    "\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test, predictions_DT))\n",
    "print(classification_report(y_test, predictions_DT))\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_DT)\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Explainer object\n",
    "explainer_DT = shap.Explainer(model_DT)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values_DT = explainer_DT.shap_values(X_test)\n",
    "\n",
    "# Visualise the first prediction's explanation\n",
    "shap.summary_plot(shap_values_DT, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiclass_summary(shap_values_list, X_test, class_names=None, max_display=None):\n",
    "    num_classes = len(shap_values_list)\n",
    "    \n",
    "    if max_display is None:\n",
    "        max_display = num_classes\n",
    "    \n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class {i}\" for i in range(num_classes)]\n",
    "    \n",
    "    for i in range(max_display):\n",
    "        shap_values_class = shap_values_list[i]\n",
    "        class_name = class_names[i]\n",
    "        \n",
    "        # Display summary plot for the class\n",
    "        shap.summary_plot(shap_values_class, X_test, show=False, plot_type='dot', title=class_name)\n",
    "        plt.title(class_name)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print average SHAP values for each feature for the class\n",
    "        print(f\"\\nAverage SHAP values for {class_name}:\\n\")\n",
    "        shap_avg = np.abs(shap_values_class).mean(axis=0)\n",
    "        for feature, value in zip(X_test.columns, shap_avg):\n",
    "            print(f\"{feature}: {value:.4f}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with shap_values_DT and X_test\n",
    "plot_multiclass_summary(shap_values_DT, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Using the Decision Tree model as an example, and targeting class 1\n",
    "features = ['minutes_since_midnight', 'device', 'text_length', 'num_pics', 'has_resp']\n",
    "\n",
    "# Create a larger figure\n",
    "fig, ax = plt.subplots(len(features), 1, figsize=(8, len(features) * 4))\n",
    "\n",
    "display = PartialDependenceDisplay.from_estimator(model_DT, X_test, features, target=1, ax=ax)\n",
    "\n",
    "# Adjust layout for better visibility\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP interaction values\n",
    "shap_interaction_values = explainer_DT.shap_interaction_values(X_test)  # using a subset for visualization\n",
    "\n",
    "# Just using the interaction values for the first class as an example\n",
    "shap_values_class_0 = shap_interaction_values[0]\n",
    "\n",
    "# Plot\n",
    "shap.dependence_plot(\n",
    "    (\"minutes_since_midnight\", \"text_length\"), \n",
    "    shap_values_class_0,\n",
    "    X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_interaction_values = explainer_DT.shap_interaction_values(X_test)\n",
    "\n",
    "# Plotting interaction values for all features\n",
    "shap.summary_plot(shap_interaction_values, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT_bi = DecisionTreeClassifier()\n",
    "\n",
    "model_DT_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "predictions_DT_bi = model_DT_bi.predict(X_test_bi)\n",
    "\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test_bi, predictions_DT_bi))\n",
    "print(classification_report(y_test_bi, predictions_DT_bi))\n",
    "\n",
    "cm_bi = confusion_matrix(y_test_bi, predictions_DT_bi)\n",
    "sns.heatmap(cm_bi, annot=True, fmt=\".0f\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Explainer object\n",
    "explainer_DT_bi = shap.Explainer(model_DT_bi)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values_DT_bi = explainer_DT_bi.shap_values(X_test_bi)\n",
    "\n",
    "# Visualise the first prediction's explanation\n",
    "shap.summary_plot(shap_values_DT_bi, X_test_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with shap_values_DT_bi and X_test_bi\n",
    "plot_multiclass_summary(shap_values_DT_bi, X_test_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = RandomForestClassifier()\n",
    "\n",
    "model_RF.fit(X_resampled, y_resampled)\n",
    "\n",
    "predictions_RF = model_RF.predict(X_test)\n",
    "\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test, predictions_RF))\n",
    "print(classification_report(y_test, predictions_RF))\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_RF)\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF_bi = RandomForestClassifier()\n",
    "\n",
    "model_RF_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "predictions_RF_bi = model_RF_bi.predict(X_test_bi)\n",
    "\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test_bi, predictions_RF_bi))\n",
    "print(classification_report(y_test_bi, predictions_RF_bi))\n",
    "\n",
    "cm_bi = confusion_matrix(y_test_bi, predictions_RF_bi)\n",
    "sns.heatmap(cm_bi, annot=True, fmt=\".0f\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LogisticAT model\n",
    "model_OL = mord.LogisticAT()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_OL.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Use the model to make predictions\n",
    "predictions_OL = model_OL.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test, predictions_OL))\n",
    "print(classification_report(y_test, predictions_OL))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions_OL)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LogisticAT model\n",
    "model_OL_bi = mord.LogisticAT()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_OL_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "# Use the model to make predictions\n",
    "predictions_OL_bi = model_OL_bi.predict(X_test_bi)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test_bi, predictions_OL_bi))\n",
    "print(classification_report(y_test_bi, predictions_OL_bi))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_bi = confusion_matrix(y_test_bi, predictions_OL_bi)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_bi, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GradientBoostingClassifier\n",
    "model_GB = GradientBoostingClassifier()\n",
    "\n",
    "model_GB.fit(X_resampled, y_resampled)\n",
    "\n",
    "predictions_GB = model_GB.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test, predictions_GB))\n",
    "print(classification_report(y_test, predictions_GB))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions_GB)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GradientBoostingClassifier\n",
    "model_GB_bi = GradientBoostingClassifier()\n",
    "\n",
    "model_GB_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "predictions_GB_bi = model_GB_bi.predict(X_test_bi)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test_bi, predictions_GB_bi))\n",
    "print(classification_report(y_test_bi, predictions_GB_bi))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_bi = confusion_matrix(y_test_bi, predictions_GB_bi)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_bi, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier\n",
    "model_XGB = xgb.XGBClassifier()\n",
    "\n",
    "# Adjust labels for XGBoost\n",
    "y_resampled_adjusted = y_resampled - 1\n",
    "y_test_adjusted = y_test - 1\n",
    "\n",
    "# Fit the model to the data\n",
    "model_XGB.fit(X_resampled, y_resampled_adjusted)\n",
    "\n",
    "# Make predictions\n",
    "predictions_XGB = model_XGB.predict(X_test)\n",
    "\n",
    "# Revert the predictions to the original scale\n",
    "predictions_XGB = predictions_XGB + 1\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score (XGBoost): ', balanced_accuracy_score(y_test, predictions_XGB))\n",
    "print(classification_report(y_test, predictions_XGB))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_XGB = confusion_matrix(y_test, predictions_XGB)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_XGB, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (XGBoost)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = model_XGB.feature_importances_\n",
    "\n",
    "# Create a mapping of feature names and their importance\n",
    "feature_importance_dict = dict(zip(X.columns, feature_importance))\n",
    "\n",
    "# Sort by importance\n",
    "sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display sorted feature importance\n",
    "for feature, importance in sorted_feature_importance:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "xgb.plot_importance(model_XGB, importance_type='weight')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier\n",
    "model_XGB_bi = xgb.XGBClassifier()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_XGB_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "# Make predictions\n",
    "predictions_XGB_bi = model_XGB_bi.predict(X_test_bi)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score (XGBoost): ', balanced_accuracy_score(y_test_bi, predictions_XGB_bi))\n",
    "print(classification_report(y_test_bi, predictions_XGB_bi))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_XGB_bi = confusion_matrix(y_test_bi, predictions_XGB_bi)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_XGB_bi, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (XGBoost)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for binary classification\n",
    "feature_importance_bi = model_XGB_bi.feature_importances_\n",
    "\n",
    "# Create a mapping of feature names and their importance for binary classification\n",
    "feature_importance_dict_bi = dict(zip(X_resampled_bi.columns, feature_importance_bi))\n",
    "\n",
    "# Sort by importance for binary classification\n",
    "sorted_feature_importance_bi = sorted(feature_importance_dict_bi.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display sorted feature importance for binary classification\n",
    "for feature, importance in sorted_feature_importance_bi:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Visualize feature importance for binary classification\n",
    "plt.figure(figsize=(10, 8))\n",
    "xgb.plot_importance(model_XGB_bi, importance_type='weight')\n",
    "plt.title('Feature Importance (Binary Classification)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical features\n",
    "cat_features = ['device', 'has_resp']\n",
    "\n",
    "# Create a CatBoost classifier\n",
    "model_CB = cb.CatBoostClassifier(cat_features=cat_features, verbose=0)\n",
    "\n",
    "# Fit the model to the data\n",
    "model_CB.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions\n",
    "predictions_CB = model_CB.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test, predictions_CB))\n",
    "print(classification_report(y_test, predictions_CB))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_CB = confusion_matrix(y_test, predictions_CB)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_CB, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (CatBoost)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CatBoost classifier\n",
    "model_CB_bi = cb.CatBoostClassifier(cat_features=cat_features, verbose=0)\n",
    "\n",
    "# Fit the model to the data\n",
    "model_CB_bi.fit(X_resampled_bi, y_resampled_bi)\n",
    "\n",
    "# Make predictions\n",
    "predictions_CB_bi = model_CB_bi.predict(X_test_bi)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print('Accuracy score: ', balanced_accuracy_score(y_test_bi, predictions_CB_bi))\n",
    "print(classification_report(y_test_bi, predictions_CB_bi))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_CB_bi = confusion_matrix(y_test_bi, predictions_CB_bi)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_CB_bi, annot=True, fmt=\".0f\", cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (CatBoost)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot ROC curves for each class in a subplot\n",
    "def plot_multiclass_roc_auc_subplot(ax, model, X_test, y_test, class_idx, model_name, lw=2):\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test == class_idx + 1, y_score[:, class_idx])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, lw=lw)\n",
    "    ax.set_title(f'{model_name} - Class {class_idx + 1}')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    return roc_auc\n",
    "\n",
    "# Initialize subplots\n",
    "fig, axes = plt.subplots(len(models), 5, figsize=(25, 25)) # Adjust figsize\n",
    "\n",
    "auc_results = {}\n",
    "\n",
    "# Loop through each model and plot ROC curves in subplots\n",
    "models = [model_DT, model_RF, model_GB, model_OL, model_XGB, model_CB]\n",
    "model_names = ['Decision Tree', 'Random Forest', 'Gradient Boosting', 'Ordered Logit', 'XGBoost', 'CatBoost']\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    avg_aucs = []\n",
    "    for i, ax in enumerate(axes[idx]):\n",
    "        auc_score = plot_multiclass_roc_auc_subplot(ax, model, X_test, y_test, i, model_names[idx])\n",
    "        avg_aucs.append(auc_score)\n",
    "    avg_auc = np.mean(avg_aucs)\n",
    "    auc_results[model_names[idx]] = avg_auc\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display overall average AUC results\n",
    "for model_name, auc_score in auc_results.items():\n",
    "    print(f\"{model_name} Overall Average AUC: {auc_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overall average AUC results\n",
    "plt.figure(figsize=(10, 6))\n",
    "model_names = list(auc_results.keys())\n",
    "avg_aucs = list(auc_results.values())\n",
    "\n",
    "# Create a bar plot for the AUC scores of each model\n",
    "sns.barplot(x=model_names, y=avg_aucs, palette=\"viridis\")\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Overall Average AUC')\n",
    "plt.title('Model AUC Comparison')\n",
    "plt.ylim([0, 1])  # Set y-axis limits to [0, 1] for AUC\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the actual AUC values on top of each bar\n",
    "for i, v in enumerate(avg_aucs):\n",
    "    plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for all models\n",
    "probs_DT = model_DT_bi.predict_proba(X_test_bi)[:, 1]\n",
    "probs_RF = model_RF_bi.predict_proba(X_test_bi)[:, 1]\n",
    "probs_GB = model_GB_bi.predict_proba(X_test_bi)[:, 1]\n",
    "probs_OL = model_OL_bi.predict_proba(X_test_bi)[:, 1]\n",
    "probs_XGB = model_XGB_bi.predict_proba(X_test_bi)[:, 1]\n",
    "probs_CB = model_CB_bi.predict_proba(X_test_bi)[:, 1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each model\n",
    "fpr_DT, tpr_DT, _ = roc_curve(y_test_bi, probs_DT)\n",
    "roc_auc_DT = auc(fpr_DT, tpr_DT)\n",
    "\n",
    "fpr_RF, tpr_RF, _ = roc_curve(y_test_bi, probs_RF)\n",
    "roc_auc_RF = auc(fpr_RF, tpr_RF)\n",
    "\n",
    "fpr_GB, tpr_GB, _ = roc_curve(y_test_bi, probs_GB)\n",
    "roc_auc_GB = auc(fpr_GB, tpr_GB)\n",
    "\n",
    "fpr_OL, tpr_OL, _ = roc_curve(y_test_bi, probs_OL)\n",
    "roc_auc_OL = auc(fpr_OL, tpr_OL)\n",
    "\n",
    "fpr_XGB, tpr_XGB, _ = roc_curve(y_test_bi, probs_XGB)\n",
    "roc_auc_XGB = auc(fpr_XGB, tpr_XGB)\n",
    "\n",
    "fpr_CB, tpr_CB, _ = roc_curve(y_test_bi, probs_CB)\n",
    "roc_auc_CB = auc(fpr_CB, tpr_CB)\n",
    "\n",
    "# Plot ROC curves in the same plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(fpr_DT, tpr_DT, color='darkorange', lw=lw, label=f'Decision Tree (AUC = {roc_auc_DT:0.2f})')\n",
    "plt.plot(fpr_RF, tpr_RF, color='blue', lw=lw, label=f'Random Forest (AUC = {roc_auc_RF:0.2f})')\n",
    "plt.plot(fpr_GB, tpr_GB, color='green', lw=lw, label=f'Gradient Boosting (AUC = {roc_auc_GB:0.2f})')\n",
    "plt.plot(fpr_OL, tpr_OL, color='red', lw=lw, label=f'Ordered Logit (AUC = {roc_auc_OL:0.2f})')\n",
    "plt.plot(fpr_XGB, tpr_XGB, color='purple', lw=lw, label=f'XGBoost (AUC = {roc_auc_XGB:0.2f})')\n",
    "plt.plot(fpr_CB, tpr_CB, color='cyan', lw=lw, label=f'CatBoost (AUC = {roc_auc_CB:0.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print out AUC scores\n",
    "print(\"Decision Tree AUC:\", roc_auc_score(y_test_bi, probs_DT))\n",
    "print(\"Random Forest AUC:\", roc_auc_score(y_test_bi, probs_RF))\n",
    "print(\"Gradient Boosting AUC:\", roc_auc_score(y_test_bi, probs_GB))\n",
    "print(\"Ordered Logit AUC:\", roc_auc_score(y_test_bi, probs_OL))\n",
    "print(\"XGBoost AUC:\", roc_auc_score(y_test_bi, probs_XGB))\n",
    "print(\"CatBoost AUC:\", roc_auc_score(y_test_bi, probs_CB))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
