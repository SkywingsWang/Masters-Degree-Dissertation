{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.graphics.regressionplots import plot_regress_exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/OneDrive - University of Warwick/Dissertation/review-Alaska_10.json'\n",
    "\n",
    "data_raw = pd.read_json(path, lines=True, encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates the presence of NA values in the data. However, as this is one of the aspects we intend to investigate, we will selectively perform data cleaning at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "- index: The index of the data.\n",
    "- user_id: The ID of the reviewer.\n",
    "- name: The name of the reviewer.\n",
    "- time: The time of the review in Unix time format.\n",
    "- rating: The rating given by the reviewer for the business.\n",
    "- text: The text of the review.\n",
    "- pics: Pictures associated with the review.\n",
    "- resp: The business response to the review, including Unix time and the text of the response.\n",
    "- gmap_id: The ID of the business."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nature of our research topic, which is to explore the influence of time of day on online ratings across different devices, we will be selecting specific data variables for further analysis. The data variables of interest include \"time,\" \"rating,\" and \"pics.\" The reason for selecting \"pics\" is due to the unfortunate inability to obtain data directly related to device types in the comments. Therefore, we need to make a crucial assumption: \n",
    "**we assume that comments with pictures are uploaded using mobile devices, while comments without pictures are uploaded using non-mobile devices.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "\n",
    "\n",
    "In this section, we will perform data preprocessing, which includes data cleaning and data transformation. Data cleaning involves handling missing values, outliers, and inconsistencies in the dataset. Data transformation may involve converting the \"pics\" data into device type data, etc. These steps allow us to make use of the available information and derive meaningful insights from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding and one-hot encoding\n",
    "data_modified = (\n",
    "    data_raw\n",
    "    .assign(time=lambda x: pd.to_datetime(x['time'], unit='ms').dt.hour)\n",
    "    # time[1,2,3] represents ['Morning', 'Noon', 'Evening']\n",
    "    .assign(time=lambda x: pd.cut(x['time'], bins=[0, 8, 16, 24], labels=[1,2,3], right=False))\n",
    "    # device[0,1] represents ['Non-mobile device', 'Mobile devices']\n",
    "    .assign(device=lambda x: x['pics'].notnull().astype(int))\n",
    "    .filter(['time', 'rating', 'device'])\n",
    ")\n",
    "\n",
    "print(data_modified[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for data distribution by time\n",
    "counts = data_modified['time'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Time')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 0.01 * counts.max(), f'{v / counts.sum() * 100:.1f}%', ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Histogram for data distribution by rating\n",
    "counts = data_modified['rating'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Rating')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 0.01 * counts.max(), f'{v / counts.sum() * 100:.1f}%', ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Histogram for data distribution by devices\n",
    "counts = data_modified['device'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Device')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Data by Device')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v + 0.01 * counts.max(), f'{v / counts.sum() * 100:.1f}%', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_modified.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test for 'time' and 'device'\n",
    "chi2_time_device, p_time_device, dof_time_device, expected_time_device = stats.chi2_contingency(pd.crosstab(data_modified['time'], data_modified['device']))\n",
    "print(\"Chi-square test result for 'time' and 'device':\")\n",
    "print(\"Chi-square statistic:\", chi2_time_device)\n",
    "print(\"P-value:\", p_time_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test for 'rating' and 'device'\n",
    "chi2_rating_device, p_rating_device, dof_rating_device, expected_rating_device = stats.chi2_contingency(pd.crosstab(data_modified['rating'], data_modified['device']))\n",
    "print(\"\\nChi-square test result for 'rating' and 'device':\")\n",
    "print(\"Chi-square statistic:\", chi2_rating_device)\n",
    "print(\"P-value:\", p_rating_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test for 'rating' and 'time'\n",
    "chi2_rating_time, p_rating_time, dof_rating_time, expected_rating_time = stats.chi2_contingency(pd.crosstab(data_modified['rating'], data_modified['time']))\n",
    "print(\"\\nChi-square test result for 'rating' and 'time':\")\n",
    "print(\"Chi-square statistic:\", chi2_rating_time)\n",
    "print(\"P-value:\", p_rating_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the chi-square test indicate that there is some degree of correlation between all three variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modified['time'] = data_modified['time'].astype(int)\n",
    "corr_matrix = data_modified.corr()\n",
    "\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_modified[['time', 'rating', 'device']]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Features'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(\"\\nMulticollinearity test:\")\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Variance Inflation Factors (VIF) for the 'time', 'rating', and 'device' variables are 4.27, 4.32, and 1.05 respectively. Typically, a VIF value greater than 5 or 10 indicates high multicollinearity. Here, none of the VIF values exceed these thresholds, suggesting that multicollinearity is not a significant concern in this dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: In the data processing phase of this project, we first cleaned and transformed the data appropriately. We then performed exploratory data analysis to visualize the distribution of ratings across different times of day and devices. Next, we conducted chi-square tests which revealed significant associations between 'time', 'device', and 'rating'. Correlation Matrix then proved these associations are not linear. A Variance Inflation Factor (VIF) test confirmed that multicollinearity was not a significant concern. This prepared the data effectively for the subsequent modeling phase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as.factor()?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y) in the balanced dataset\n",
    "X = data_modified.drop('rating', axis=1)\n",
    "y = data_modified['rating']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2221877)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Balancing\n",
    "\n",
    "According to the \"rating distribution\", it can be seen that there is a significant data imbalance between the different ratings, and stratified sampling may not be able to address this issue. Therefore, we plan to employ undersampling as a strategy to achieve balance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate features (X) and target variable (y)\n",
    "# X = data_modified.drop('rating', axis=1)\n",
    "# y = data_modified['rating']\n",
    "\n",
    "# Apply SMOTEENN using training sets\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Create a new balanced dataframe\n",
    "# balanced_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "# balanced_data['rating'] = y_resampled\n",
    "\n",
    "# # Check the balanced distribution of ratings\n",
    "# print(balanced_data['rating'].value_counts())\n",
    "\n",
    "# Check the balanced distribution of ratings\n",
    "print(pd.Series(y_resampled).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.combine import SMOTETomek\n",
    "\n",
    "# # Separate features (X) and target variable (y)\n",
    "# X = data_modified.drop('rating', axis=1)\n",
    "# y = data_modified['rating']\n",
    "\n",
    "# # Apply SMOTETomek\n",
    "# smote_tomek = SMOTETomek(random_state=42)\n",
    "# X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "\n",
    "# # Create a new balanced dataframe\n",
    "# balanced_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "# balanced_data['rating'] = y_resampled\n",
    "\n",
    "# # Check the balanced distribution of ratings\n",
    "# print(balanced_data['rating'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the training and testing sets\n",
    "print(\"Training set shape:\", X_resampled.shape, y_resampled.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression model\n",
    "model = LinearRegression()\n",
    "result = model.fit(X_resampled, y_resampled)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# model evaluation\n",
    "print('mean_squared_error : ', mean_squared_error(y_test, predictions))\n",
    "print('mean_absolute_error : ', mean_absolute_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge predictions and y_test into a single data frame\n",
    "plot_regression = pd.DataFrame({'predictions': predictions, 'actual_rating': y_test})\n",
    "\n",
    "# Create a scatter plot with linear regression line\n",
    "sns.lmplot(x=\"predictions\", y=\"actual_rating\", data=plot_regression, scatter_kws={\"alpha\": 0.5})\n",
    "plt.xlabel('Predicted Rating')\n",
    "plt.ylabel('Actual Rating')\n",
    "plt.title('Predicted vs Actual Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot does not form a significant linear relationship between the predicted and actual values, which confirms my previous correlation coefficient calculations that there is no significant linear relationship between my variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建模型\n",
    "model = LogisticRegression()\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "print('Accuracy score: ', accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# 可视化混淆矩阵\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 创建多项逻辑回归模型，设置 multi_class 参数为 'multinomial'\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score: ', accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# # Fit the polynomial regression model\n",
    "# degree=2\n",
    "# polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "# polyreg.fit(X_train, y_train)\n",
    "# predictions = polyreg.predict(X_test)\n",
    "\n",
    "# # Model evaluation\n",
    "# print('mean_squared_error : ', mean_squared_error(y_test, predictions))\n",
    "# print('mean_absolute_error : ', mean_absolute_error(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 创建模型\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "print('Accuracy score: ', accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# 可视化混淆矩阵\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 创建模型\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "print('Accuracy score: ', accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# 可视化混淆矩阵\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate features (X) and target variable (y) in the balanced dataset\n",
    "# X = balanced_data.drop('rating', axis=1)\n",
    "# y = balanced_data['rating']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2221877)\n",
    "\n",
    "# Combine the features and target variable into a single DataFrame\n",
    "data_train = X_resampled.copy()\n",
    "data_train['rating'] = y_resampled\n",
    "\n",
    "# Define the formula\n",
    "formula = 'rating ~ C(time) + C(pics)'\n",
    "\n",
    "# Fit the model on the training set\n",
    "model = ols(formula, data=data_train).fit()\n",
    "\n",
    "# Perform ANOVA analysis on the training set\n",
    "anova_results = sm.stats.anova_lm(model)\n",
    "print(anova_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the residual plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plot_regress_exog(model, 'C(pics)[T.1]', fig=fig)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the p-values of the two columns are very optimistic, the degrees of freedom of the residuals are too large."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Probability Model (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lpm_mod = sm.OLS(y_train, X_train)\n",
    "# lpm_res = lpm_mod.fit()\n",
    "# print(\"Parameters: \", lpm_res.params[:-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mapping score values from 1-5 to a range between 0 and 1\n",
    "# y_train_mapped = np.interp(y_train, (1, 5), (0, 1))\n",
    "\n",
    "# # Model fitting using the mapped y_train_mapped\n",
    "# logit_mod = sm.Logit(y_train_mapped, X_train)\n",
    "# logit_res = logit_mod.fit(disp=0)\n",
    "# print(logit_res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_res.pred_table()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marginal Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# margeff = logit_res.get_margeff()\n",
    "# print(margeff.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(logit_res.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlogit_mod = sm.MNLogit(y_train, X_train)\n",
    "# mlogit_res = mlogit_mod.fit()\n",
    "# print(mlogit_res.params)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod_nbin = sm.NegativeBinomial(y_train, X_train)\n",
    "# res_nbin = mod_nbin.fit(disp=False)\n",
    "# print(res_nbin.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlogit_res = mlogit_mod.fit(method=\"bfgs\", maxiter=250)\n",
    "# print(mlogit_res.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparisons\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
